% !TEX root = ozan_sener_thesis.tex

% Define the robot perception problem

\todo{not sure}
Consider a typical robotics system in which humans and robots need to work side-by-side performing a high-level tasks like cooking or household repairs. Robots need to have rich set of sensors and perception algorithms which can detect and localize objects in the environment, understand the humans and their activities, reason about the temporal context and the hierarchy of the high-level and fine-grained tasks. All these problems are fundamentals of robot perception and not only studied in robotics literature but also in computer vision, natural language processing and artificial intelligence literatures.




\todo{}

% Going from expert to data
\todo{okay as 3rd paragraph}
% This is at 150% verbrosity level.
In the early ages of artificial intelligence, the popular approach to the problem of computer vision and robot perception was designing very high detailed expert curated knowledge. For example, we used geometric primitives and geometric algorithms designed by experts \cite{cylinder}. 
This way of looking to the problem was a reflection of the way we accessed information in our lives. For example, we used expert curated information sources like encyclopedia to learn hard concepts and used recipe books to learn cooking. Recently; we moved from very small scale expert curated information sources to large-scale crowd generated ones. For example, Wikipedia replaced our encyclopedias with many order of magnitude more entries. Similarly, we moved away from recipe books to crowd-generated video databases like YouTube\cite{youtube}. Google Trends\cite{google_trends} indicates that in the year of 2005 number of Google searches for \emph{cookbooks} were $1.56$ times larger than the number of searches for \emph{cooking videos}. In the year 2016, the number of searches for \emph{cooking videos} is $8.6$ times larger than that of \emph{cookbooks}. This behavior is mostly due to the large volume of cooking videos available on the internet. Following this trend, computer vision researchers successfully collected very large datasets of images and learned very expressive object detectors\cite{rcnn}, object classifiers \cite{alexnet,vggnet,googlenet,residual}. However, we did not see a similar trend in robot perception even though computer vision and robot perception are very similar problems. We discuss the possible reasons for this trend below.

% Limitations of data-driven methods
The success of data-driven methods repeatedly proved that a success is only possible with very large-scale data. This behavior is typically attributed to the Zipf law governing the multi-media information we generate. If we collect a random sample of multi-media information from the web or nature, and draw the histogram of each object/activity/concept occurring in this set; the resulting distribution would follow the Zipf law.  ence the amount of data needed to cover all classes/modes, scales exponentially with the number of classes/modes. Although recognizing a few object categories is enough for many applications like web-search, image retrieval etc., the robot perception has very bigger scale. For example, for a robot to semantically understand a visual scene; it needs to parse the scene into meaningful objects and needs to find humans and their activities resulting. Such a task requires a joint understanding of various concepts resulting in a space of dimension significantly larger, making the manual data collection and human supervision intractable. Hence, we try to address the challenge of scaling multi-modal robot related data collection with the help of crowd and robots. Moreover, we also try to overcome lack of human supervision by designing un/weekly-supervised algorithms.

% Robot Perception is not computer vision
Perception and planning are among the two fundamental sub-modules of any robotics system. Robots needs powerful perception algorithms which can convert raw sensory readings into semantically meaningful \emph{representations}. Given this representations, planner sub-module needs to come up with a physically plausible plan for a robot to execute. Although robot perception is in principle aims to convert sensory input (\eg videos) into semantic concepts (\eg humans, objects, activitities), developing it in isolation is not a scalable approach. We want our robots to robustly perform in various environments of different complexities. We even want them to perform in environments we have never seen in our knowledge bases. Hence, we want our \emph{representations} to handle uncertainty. Moreover, we also want these representations to be understandable by planning systems to be useful in overall robotics scenario. Hence, we believe the problem of robot perception learning is indeed a problem of learning \emph{actionable representations} and we treat it so.

Following the aforementioned major challenges of scalable data collection, handling lack of supervision and learning actionable representations, we try to answer the following questions in this thesis;
\begin{itemize}
\item How can we tractably collect a large knowledge base of actionable physical information?
\item How can we formulate and solve the problem of uncertainty aware perception algorithms?
\item How can we learn actionable representations from a dataset with no/limited supervision?
\item How can we overcome the shift between representations designed for planning and perception systems? 
\end{itemize}

Fortunately all of these problems are somewhat linked with each other. Moreover, our key insight to all these questions lies in exploiting the \emph{structure} in the data. Using sparsity of the structured spaces, we develop algorithms which can handle uncertainty, using low-dimensional substructure of the structured spaces, we develop algorithms which can handle lack of supervision even at the extreme case of unsupervised learning. 


\section{Sensing at Scale: \\ Scaling the actionable knowledge bases}
Robotics systems composed of many sub-systems and they are related in various ways creating a necessity of knowledge bases spanning different modalities and domains. For example, consider a very basic example of concept of a kettle. We need to relate the kettle with a physical entity which can boil water for planning purpose, similarly we need a very detailed visual description of the kettle for perception subsystem to recognize and locate them, moreover we also need to understand kettle in full geometric form for grasping and manipulation algorithms to work. Hence, most of the existing knowledge bases\cite{yago2007, ferrucci2012a, freebase2008} are very little use; since, they do not jointly consider modalities and they are limited to a single context.

In order to solve this problem, we introduce a knowledge engine, which learns and shares structured knowledge representations, for robots to carry out a variety of tasks. Our knowledge engine is designed to solve the unique challenges due to dealing with multiple modalities including symbols, natural language, haptic senses, robot trajectories, visual features and many others. The \textit{knowledge} we curated through crowd-sourcing comes from multiple sources including physical interactions that robots have while performing tasks (perception, planning and control), knowledge bases from the Internet and learned representations from several robotics research groups. 

In Chapter \ref{rb}, we discuss various technical aspects and associated challenges such as modeling the correctness of knowledge, inferring latent information and formulating different robotic tasks as  queries to the knowledge engine. We describe the system architecture and how it supports different mechanisms for users and robots to interact with the engine. We give very detailed treatment about how can we design a multi-media knowledge base which can scale to thousands of videos as we further use in Chapter~\ref{repr}. This knowledge engine is developed in collaboration with many researchers as a part of a bigger project  \footnote{In collaboration with Saxena, Jain, Jami, Misra, Koppula as part of http://robobrain.me}, and our contribution mostly lies in designing a large-scale multimedia system architecture and  integrating existing knowledge bases. 


\section{Perception at Scale: \\  Handling unknowns of robot perception for action}
Context is a key to many robot perception tasks. For example, it is crucial to model relationships between objects if the final aim is parsing the visual scene into humans and objects \cite{KoppulaIJRR2012}. Similarly, a context of humans is critical to understand activities \cite{hemaIJRR, hall}. One of the very successful ways of modeling context is using graphical models. 

Although graphical models are very powerful tools to model context, inference in such a system typically follows finding the Maximum-a-Posterior solution via solving a combinatorial optimization problem . Hence, such system generates a single prediction with possible errors. The errors typically come from both known and unknown unknowns of the systems. The error can be because of a behavior never seen in the dataset or because of a behavior which very rarely happens in the data and considered to be a noise by learning algorithms. The solution this problem is designing representations which can handle uncertainty caused by known and unknown unknowns.

In Chapter \ref{rcrf}, we present a new recursive algorithm that we call Recursive Conditional Random Field (rCRF) which can compute a belief over a temporal CRF model accurately representing uncertanity. We extend the graphical models to temporal structures which can model the uncertainty by estimating a full believe over predicted variables. We only use a structured diversity in our formulation and present a computationally tractable inference and learning algorithm based on Bayesian filtering and combinatorial diverse-$M$-best construction. We further apply our model on the problem of efficiently computing beliefs over future human activities from RGB-D videos. We present our extensive experimentation on human activity anticipation setup and also briefly describe another successful application of our algorithm on the problem of parsing large point clouds.


\section{Learning at Scale: \\ Learning actionable representations with no-supervision}
After we develop our large-scale actionable knowledge base \emph{RoboBrain}, one observation we had was scarcity of supervision. The powerful knowledge we collected from YouTube in terms of instructional videos had no supervision on them, moreover, existing supervision was very sparse covering only the subset of the videos. Moreover, most of the videos also has human speech in it which requires a multi-modal approach.

In order to handle the challenge of learning with limited/no supervision over multiple modalities, we rely on the underlying structure of the data as well. For example, consider the YouTube videos. They are typically generated by humans to communicate their knowledge. Moreover, human communication typically has an underlying structure, with a starting point, ending, and certain objective steps between them. In order to exploit this structure, we propose a method for parsing a video into such semantic steps in an unsupervised way in Chapter~\ref{repr}. We accomplish this using both visual and language cues in a joint generative model. Our method can also provide a textual description for each of the identified semantic steps and video segments. 

\section{Sharing at Scale: \\ Learning to close the gap between domains}
Main motivation behind our large-scale actionable knowledge base \emph{RoboBrain} is letting robots share knowledge with each other. This is critical since robotics system includes many sub-systems and most of the research is contained within one of the sub-domains. Hence, scaling is only possible when we close the gap between domains. For example, most of the available robot perception systems are trained using real camera/sensor recordings. On the other hand, planning algorithms mostly uses symbolic representations or very simple visual images like computer generated images in video game logs as in \cite{tellmedave}.

In order to handle multiple domains, we also rely on structure. Our construction was simply using the available supervised domains to supervise rest of the domains. In other words, we approach the problem from a transductive perspective. We incorporate the domain shift and the transductive target inference into our framework by jointly solving for an asymmetric similarity metric and the optimal transductive target label assignment. Key insight to solve this metric learning with limited supervision was enforcing structural consistency in unsupervised domains. We also show that our model can easily be extended for deep feature learning in order to learn features which are discriminative in the target domain. We show that our method is capable of linking artificial computer generated images planner algorithms use with actual images from robot perception datasets in Chapter~\ref{repr2}

\section{First published appearances of the contributions}

Most of the contributions described in this dissertation have first appeared as the following publications.
\begin{itemize}
\item Chapter 2: Saxena, Jain, Sener, Jami, Misra, and Koppula \cite{robobrain}
\item Chapter 3: Sener and Saxena \cite{rcrf}; Armeni, Sener, Zamir, Jiang and Savarese \cite{cvpr_iro}
\item Chapter 4: Sener, Zamir, Savarese and Saxena \cite{iccv_ozan}; Sener, Wu, Zamir, Savarese and Saxena \cite{ijcv_ozan}; Sener, Song, Saxena, Saverese \cite{da_ozan}.
\end{itemize}

The following research works are applications or are related to my thesis, but are not fully presented in this thesis:
\begin{itemize}
\item {3D detection.}  This is an application of our approach in Chapter ~\ref{} and is described completely in \cite{xx}
\item {Graphical Models.} 
\end{itemize}
