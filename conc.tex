We started this dissertations with the question of ``Can we extract structured and physically grounded knowledge from the publicly available information on the web''. Although our answer to this questions is not a definite ``yes'' yet, we showed a very promising direction following a study of knowledge-base creation, representation learning and explicit uncertainty modeling. 

Our first major step was identifying key challenges preventing us, roboticists, from using  existing large-scale knowledge bases. After identifying these challenges, we designed and developed a new knowledge based carefully tailored for robots \emph{www.robobrain.me}. 

Obtaining a large-scaled knowledge, our next step was developing large-scale machine learning algorithms which can learn meaningful representations from the available information. Considering the cost of labelling such a large knowledge base, we designed unsupervised video parsing algorithms. Our proposed algorithm discovered emerging patterns from the videos. It learned objects and their dynamics as well as the activities humans perform.

Learning semantic representations of objects and activities from large-scale collection of videos, resulted in an ambitious question. ``Can we go from instructional videos to physical robot plans?''. Main rationale behind asking this question was the availability of very large collections of instructional videos in YouTube. For example, there are 200 thousand videos only to describe \emph{how to make a poached egg?}. Such an end-to-end system requires solving the domain shift between planning knowledge and perception knowledge. Moreover, we proposed a domain adaptation algorithm for this purpose and showed a successful robot plans generated completely from large-scale data with no human supervision.

Our final step was handling the common challenge of all data-driven systems, unknowns. Since none of the knowledge-bases are complete, all data-driven systems will eventually face an environment they never saw. We believe the key to handle this is explicitly modeling uncertainty. We proposed an estimation algorithm to explicitly model uncertainty. 

Using all these methods, we showed a successful demonstrations of robots directly going from unsupervised large-scale data to physically plausible plans in collaboration with humans. Our demonstrations showed a promising direction but we believe there is still a long way to go. In the rest of this section, we discuss the next steps in this ambitious aim.

One big challenge is natural language processing. Within the scope of this thesis, we used very primitive algorithms to parse and understand natural language. We even used a limited human supervision in some of the applications. We believe it is possible to develop very expressive natural language representations for robotics. The key difference to existing natural language processing methods is the representations need to be multi-domain. For example, the representation to ``Cup'' not only include its language properties but also physical and visual properties. In other words, we need joint representations of the words to be used in robotics.

Yet another challenge is the necessity to model common-sense. We all know that butter is most likely to be found in the fridge and screwdriver is probably in the garage. However, we never explicitly mention these kind of knowledge neither in our written text nor in our instructional videos. Hence, we need algorithms which can learn these common-sense concepts. We believe the path to learning common-sense knowledge is following the way babies learn. Babies learn all these concepts by constantly watching adults and also physically playing with them. Hence, we hypothesize this is possible by personal robots constantly gathering information from our houses and sharing with them. Sharing is a key here since it is not tractable for a single robot to learn the entire human common-sense in a tractable amount of time. 

In summary, we studied the problem of robot perception in this dissertation with a focus on large-scale learning approaches. We showed that large-scale data is extremely useful for robots as long as it satisfy a few properties like being multi-modal, multi-domains and physically grounded. We further showed that visual data have strong structural priors like structural diversity, consistency and hierarchy. We developed unsupervised and multi-domain machine learning algorithms using this priors resulting in state-of-the-art performance in many robot perception tasks. After all, we believe we will be able to answer the aforementioned question with a definite ``yes'' as long as we continue to exploit the structure and share the representations.

